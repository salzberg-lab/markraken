{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"test marker representation on sample sequences\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import mmh3\n",
    "import pickle\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data paths\n",
    "data_dir = '/ccb/salz4-4/markus/markraken/data'\n",
    "\n",
    "# seq_pickle_path = os.path.join(data_dir, 'DB.pkl')\n",
    "# HPC_pickle_path = os.path.join(data_dir, 'DB_HPC.pkl')\n",
    "# hashtable_path = os.path.join(data_dir, 'index.pkl')\n",
    "\n",
    "seq_pickle_path = os.path.join(data_dir, 'DB_full.pkl')\n",
    "HPC_pickle_path = os.path.join(data_dir, 'DB_HPC_full.pkl')\n",
    "hashtable_path = os.path.join(data_dir, 'index_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(seq_pickle_path, 'rb') as f:\n",
    "    seq_list = pickle.load(f)\n",
    "\n",
    "with open(HPC_pickle_path, 'rb') as f:\n",
    "    HPC_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    }
   ],
   "source": [
    "n_markers = 1000 # number of markers to use for representation of sequence as markers, Shasta default = 8000\n",
    "marker_length = 10 # length of marker sequence, Shasta default = 10\n",
    "\n",
    "def reverse_complement(nucseq):\n",
    "    complement = {'A':'T',\n",
    "                  'T':'A',\n",
    "                  'G':'C',\n",
    "                  'C':'G'}\n",
    "    reverse_complement_nucseq = ''.join([complement[x] for x in nucseq[::-1]])\n",
    "    return reverse_complement_nucseq\n",
    "\n",
    "# randomly select marker set without replacement, surprisingly complex to vectorize, works fast enough like this\n",
    "random.seed(2019)\n",
    "marker_set = set()\n",
    "while len(marker_set) < n_markers:\n",
    "    marker = [random.sample('ATCG', 1)[0]]\n",
    "    for i in range(marker_length-1):\n",
    "        nucs = {'A', 'T', 'C', 'G'}\n",
    "        nucs.remove(marker[-1])\n",
    "        marker.append(random.sample(nucs, 1)[0])\n",
    "    seq = ''.join(marker)\n",
    "    marker_set.add(seq)\n",
    "    marker_set.add(reverse_complement(seq))\n",
    "\n",
    "marker_list = list(marker_set)\n",
    "print(len(marker_list)) # because we also store reverse complement, this may be off by one, but this shouldnt matter really"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_c_list = []\n",
    "for m in marker_list:\n",
    "    re_c = re.compile(m)\n",
    "    re_c_list.append(re_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56217\n",
      "798\n",
      "CPU times: user 234 ms, sys: 1.87 ms, total: 236 ms\n",
      "Wall time: 235 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def markerize(HPC_seq):    \n",
    "    d = []\n",
    "    for i, re_c in enumerate(re_c_list):\n",
    "        idx = [s.start(0) for s in re_c.finditer(HPC_seq)]\n",
    "        for x in idx:\n",
    "            d.append([x,i])\n",
    "    \n",
    "    marker_seq = np.zeros(len(HPC_seq), dtype=int)-1\n",
    "    for marker_loc in d:\n",
    "        marker_seq[marker_loc[0]] = marker_loc[1]\n",
    "    \n",
    "    marker_seq_clean = marker_seq[np.where(marker_seq!=-1)]\n",
    "    \n",
    "    return list(marker_seq_clean)\n",
    "\n",
    "foo = markerize(HPC_list[1])\n",
    "print(len(HPC_list[1]))\n",
    "print(len(foo))\n",
    "# print(foo)\n",
    "\n",
    "# this code is so dirty I might as well write it in C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_threads = 20\n",
    "\n",
    "# markerize all HPC seqs\n",
    "p = mp.Pool(n_threads)\n",
    "markerized_list = p.map(markerize, HPC_list)\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to tuple to allow hashing\n",
    "markerized_chrlist = [''.join([chr(s) for s in x]) for x in markerized_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# extract marKmers (short kmers with much larger alphabet) from marker sequences\n",
    "# marKmer uniqueness is roughly equivalent to a normal kmer of length = marKmer_length*marker_length\n",
    "# e.g. with marker length 10, marKmer 3 ~ kmer 30\n",
    "marKmer_length = 4\n",
    "hash_seed = 2019\n",
    "\n",
    "marKmer_hashtable = dict()\n",
    "for i, m in enumerate(markerized_chrlist): # i is stand-in for true taxid, TODO handle taxid with LCA\n",
    "    marKmer_set = set()\n",
    "    for j in range(len(m)):\n",
    "        subseq = m[j:j+marKmer_length] # this might be slow, TODO check if this slices or copies\n",
    "        marKmer_set.add(subseq) # adding to set is O(1)\n",
    "        \n",
    "    hashlist = [mmh3.hash(x, hash_seed) for x in list(marKmer_set)]\n",
    "    tmp_dict = dict(zip(hashlist, [i]*len(hashlist)))\n",
    "    marKmer_hashtable.update(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # extract marKmers (short kmers with much larger alphabet) from marker sequences\n",
    "# # marKmer uniqueness is roughly equivalent to a normal kmer of length = marKmer_length*marker_length\n",
    "# # e.g. with marker length 10, marKmer 3 ~ kmer 30\n",
    "# marKmer_length = 3\n",
    "# hash_seed = 2019\n",
    "\n",
    "# marKmer_hashtable = dict()\n",
    "# for i, m in enumerate(markerized_chrlist): # i is stand-in for true taxid, TODO handle taxid with LCA\n",
    "#     marKmer_set = set()\n",
    "#     for i in range(len(m)):\n",
    "#         subseq = m[i:i+marKmer_length] # this might be slow, TODO check if this slices or copies\n",
    "#         marKmer_set.add(subseq) # adding to set is O(1)\n",
    "#     for m in marKmer_set:\n",
    "#         h = mmh3.hash(m, hash_seed)\n",
    "#         marKmer_hashtable.update({h:i}) # TODO check if adding to dict is O(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index\n",
    "hashtable_path = os.path.join(data_dir, 'index_full_m1000.pkl')\n",
    "\n",
    "with open(hashtable_path, 'wb') as f:\n",
    "    pickle.dump(marKmer_hashtable, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.110610708 gigabytes for index\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot serialize a string larger than 4GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e91560515162>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msingleseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarKmer_hashtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gigabytes for index'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# guaranteed correct or overestimate of object size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingleseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gbp of sequence'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m: cannot serialize a string larger than 4GiB"
     ]
    }
   ],
   "source": [
    "# examine compression ratio\n",
    "singleseq = ''.join(seq_list)\n",
    "print(len(pickle.dumps(marKmer_hashtable))/1e9, 'gigabytes for index') # guaranteed correct or overestimate of object size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.418361975 Gbp of sequence\n"
     ]
    }
   ],
   "source": [
    "print(len(singleseq)/1e9, 'Gbp of sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markerize_chr(HPC_seq):\n",
    "    marker_seq = []\n",
    "    for i in range(len(HPC_seq)):\n",
    "        subseq = HPC_seq[i:i+marker_length]\n",
    "        try:\n",
    "            idx = marker_list.index(subseq)\n",
    "            marker_seq.append(chr(idx))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return marker_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marKmerize(marker_chrlist, marK):\n",
    "    marKmer_set = set()\n",
    "    for i in range(len(marker_chrlist)-marK):\n",
    "        marKmer = ''.join(marker_chrlist[i:i+marK])\n",
    "        marKmer_set.add(marKmer)\n",
    "    return marKmer_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "148\n",
      "144\n",
      "[1, 33, 1, 8718, 1, 33, 1, 33, 1, 1, 8718, 8718, 33, 1, 33, 1, 33, 1, 33, 1, 8718, 1, 1, 1, 3457, 8718, 1, 33, 1, 1, 8718, 1, 1, 8718, 33, 33, 1, 8718, 1, 33, 1, 8718, 8718, 8718, 1, 1, 8718, 1, 1, 1, 8718, 1, 1, 8718, 1, 1, 33, 8718, 1, 1, 1, 1, 1, 8718, 9474, 14450, 1, 8718, 1, 1, 8718, 1, 33, 1, 33, 1, 1, 1, 8718, 1, 33, 1, 8718, 8718, 8718, 1, 8718, 1, 1, 1, 8718, 8718, 33, 1, 1, 8718, 1, 8718, 1, 8718, 1, 1, 16986, 1, 1, 1, 8718, 8718, 1, 1, 1, 1, 8718, 8718, 8718, 1, 8718, 8718, 1, 8718, 33, 1, 33, 33, 16175, 1, 33, 33, 1, 1, 33, 33, 1, 33, 8718, 1, 1, 8718, 1, 1, 1, 1, 33, 33]\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "offset = 0\n",
    "readlength = 10000\n",
    "marK = marKmer_length # like k for kmers, ensure this is same as used in index\n",
    "\n",
    "\n",
    "sample_HPC = HPC_list[i][offset:offset+readlength]\n",
    "print(len(sample_HPC))\n",
    "\n",
    "sample_marker = markerize_chr(sample_HPC)\n",
    "print(len(sample_marker))\n",
    "\n",
    "sample_marKmer = marKmerize(sample_marker, marK)\n",
    "print(len(sample_marKmer))\n",
    "\n",
    "i_predict_list = []\n",
    "for m_str in sample_marKmer:\n",
    "    m_hash = mmh3.hash(m_str, hash_seed)\n",
    "    try:\n",
    "        i_predict = marKmer_hashtable[m_hash]\n",
    "        i_predict_list.append(i_predict)\n",
    "    except:\n",
    "        pass # avoid throwing error if hash isnt in table\n",
    "print(i_predict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO examine accuracy in perfect reads\n",
    "#TODO examine accuracy as a function of sequencing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenlist = [len(x) for x in markerized_list]\n",
    "plt.hist(lenlist)\n",
    "plt.show()\n",
    "print(np.sum(np.asarray(lenlist)>10000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
